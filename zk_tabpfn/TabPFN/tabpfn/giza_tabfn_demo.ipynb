{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Scalable ZKML \"training\" via Prior Fitted Networks\n",
    "\n",
    "\n",
    "### What if we could prove what training data was used to make predictions, in the same way we prove inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\n",
    "https://github.com/automl/TabPFN\n",
    "\n",
    "\n",
    "#### From the abstract\n",
    "We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN performs in-context learning (ICL), it learns to make predictions using sequences of labeled examples (x, f(x)) given in the input, without requiring further parameter updates. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to 230× speedup. This increases to a 5 700× speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML. We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at this https URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api-dev.gizatech.xyz\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "from giza_actions.action import action, Action\n",
    "from giza_actions.task import task\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from scripts.model_builder import get_default_spec, save_model, load_model_only_inference\n",
    "from scripts.transformer_prediction_interface import transformer_predict, get_params_from_config, TabPFNClassifier\n",
    "\n",
    "from datasets import load_openml_list, open_cc_dids, open_cc_valid_dids, test_dids_classification\n",
    "\n",
    "from scripts import tabular_metrics\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from functools import partial\n",
    "import tabpfn.encoders as encoders\n",
    "\n",
    "from transformer import TransformerModel\n",
    "\n",
    "import uuid\n",
    "\n",
    "os.environ['GIZA_API_HOST'] = 'https://api-dev.gizatech.xyz'\n",
    "print(os.environ['GIZA_API_HOST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=f'get_dataset')\n",
    "def get_dataset():\n",
    "    base_path = '.'\n",
    "    max_samples = 1000\n",
    "    bptt = 10000\n",
    "    test_datasets= load_openml_list([11], multiclass=True, shuffled=True, filter_for_nan=False, max_samples = max_samples, num_feats=100, return_capped=True)[0]\n",
    "    ds = test_datasets[0]\n",
    "    print(f'Evaluation dataset name: {ds[0]} shape {ds[1].shape}')\n",
    "    xs, ys = ds[1].clone(), ds[2].clone()\n",
    "    print(xs.shape)\n",
    "    print(ys.shape)\n",
    "    eval_position = xs.shape[0] // 2\n",
    "    print(eval_position)\n",
    "    train_xs, train_ys = xs[0:eval_position], ys[0:eval_position]\n",
    "    test_xs, test_ys = xs[eval_position:], ys[eval_position:]\n",
    "    return train_xs, train_ys, test_xs, test_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_only_inference_for_onnx(path, filename, device):\n",
    "    \"\"\"\n",
    "    Loads a saved model from the specified position. This function only restores inference capabilities and\n",
    "    cannot be used for further training.\n",
    "    \"\"\"\n",
    "\n",
    "    model_state, optimizer_state, config_sample = torch.load(os.path.join(path, filename), map_location='cpu')\n",
    "\n",
    "    if (('nan_prob_no_reason' in config_sample and config_sample['nan_prob_no_reason'] > 0.0) or\n",
    "        ('nan_prob_a_reason' in config_sample and config_sample['nan_prob_a_reason'] > 0.0) or\n",
    "        ('nan_prob_unknown_reason' in config_sample and config_sample['nan_prob_unknown_reason'] > 0.0)):\n",
    "        encoder = encoders.NanHandlingEncoder\n",
    "    else:\n",
    "        encoder = partial(encoders.Linear, replace_nan_by_zero=True)\n",
    "\n",
    "    n_out = config_sample['max_num_classes']\n",
    "\n",
    "    device = device if torch.cuda.is_available() else 'cpu:0'\n",
    "    encoder = encoder(config_sample['num_features'], config_sample['emsize'])\n",
    "\n",
    "    nhid = config_sample['emsize'] * config_sample['nhid_factor']\n",
    "    y_encoder_generator = encoders.get_Canonical(config_sample['max_num_classes']) \\\n",
    "        if config_sample.get('canonical_y_encoder', False) else encoders.Linear\n",
    "\n",
    "    assert config_sample['max_num_classes'] > 2\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction='none', weight=torch.ones(int(config_sample['max_num_classes'])))\n",
    "    with torch.no_grad():\n",
    "        model = TransformerModel(encoder, n_out, config_sample['emsize'], config_sample['nhead'], nhid,\n",
    "                                config_sample['nlayers'], y_encoder=y_encoder_generator(1, config_sample['emsize']),\n",
    "                                dropout=config_sample['dropout'],\n",
    "                                full_attention=True,\n",
    "                                num_global_att_tokens=None,\n",
    "                                )\n",
    "\n",
    "        # print(f\"Using a Transformer with {sum(p.numel() for p in model.parameters()) / 1000 / 1000:.{2}f} M parameters\")\n",
    "\n",
    "        model.criterion = loss\n",
    "        module_prefix = 'module.'\n",
    "        model_state = {k.replace(module_prefix, ''): v for k, v in model_state.items()}\n",
    "        model.load_state_dict(model_state)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        return model # no loss measured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=f'generate_onnx_model')\n",
    "def generate_onnx_model():\n",
    "    path = '../tabpfn/models_diff/'\n",
    "    filename = 'prior_diff_real_checkpoint_n_0_epoch_42.cpkt'\n",
    "    device = 'cpu'\n",
    "    model = load_model_only_inference_for_onnx(path, filename, device)\n",
    "    \n",
    "    torch.manual_seed(420)\n",
    "\n",
    "    x =  torch.randn(625, 3, 100)\n",
    "    y = torch.randn(312,3)\n",
    "\n",
    "    d = (x,y)\n",
    "    model.eval()\n",
    "\n",
    "    input_names = [\"src\",\"onnx::Unsqueeze_1\"]\n",
    "    torch.onnx.export(model, (d, ), \"tabpfn.onnx\", input_names=input_names, export_params=True, opset_version=13, do_constant_folding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabPFN Sklearn interface\n",
    "\n",
    "- The fit function does not perform any computations, but only saves the training data. Computations are only done at inference time, when calling predict.\n",
    " \n",
    "- Note that the presaved models were trained for up to 100 features, 10 classes and 1000 samples. While the model does not have a hard bound on the number of samples, the features and classes are restricted and larger sizes lead to an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=f'init inference_only_model')\n",
    "def init_inference_only_model():\n",
    "    return TabPFNClassifier(device='cpu', only_inference=True)\n",
    "\n",
    "@task(name=f'fit')\n",
    "def fit(model, train_xs, train_ys):\n",
    "    '''Initializes the TabPFN class with the training data, following their sklearn interface. \n",
    "       Note, there is NOT any model interaction happening here. :)'''\n",
    "    print(\"Setting up TabPFN with training data context\")\n",
    "    return model.fit(train_xs, train_ys)\n",
    " \n",
    "@task(name=f'predict')\n",
    "def predict(model, test_xs, with_onnx=True):\n",
    "    '''The TabPFN workflow is enhanced to use the GizaModel() if with_onnx=True.\n",
    "       Please see transformer_prediction_interface.py for implementation details.'''\n",
    "    print(\"prediction!\")\n",
    "    return model.predict_proba(test_xs, with_onnx=True)\n",
    "\n",
    "    \n",
    "@task(name=f'eval')\n",
    "def auc_eval(test_ys, prediction):\n",
    "    roc = tabular_metrics.auc_metric(test_ys, prediction)\n",
    "    print('AUC', float(roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@action(name=\"Action: Test TabPFN\", log_prints=True)\n",
    "def run_model():\n",
    "    \n",
    "    #loads in TabPFNClassifier with sklearn interface and downloads weights\n",
    "    model = init_inference_only_model()\n",
    "    \n",
    "    generate_onnx_model()\n",
    "    \n",
    "    train_xs, train_ys, test_xs, test_ys = get_dataset()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # initializing only, no actual model interaction here!\n",
    "    model = fit(model, train_xs, train_ys)\n",
    "    \n",
    "    # makes prediction on test_xs using approximate bayesian inference with train data 'in-context'\n",
    "    prediction = predict(model, test_xs, with_onnx=True)\n",
    "    \n",
    "    auc_eval(test_ys, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:00:29.872 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Created flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'hilarious-pegasus'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'Action: Test TabPFN'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:00:29.872 | \u001b[36mINFO\u001b[0m    | Created flow run\u001b[35m 'hilarious-pegasus'\u001b[0m for flow\u001b[1;35m 'Action: Test TabPFN'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:00:29.880 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - View at <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://actions-server-tyfar2-dblzzhtf5q-ew.a.run.app/flow-runs/flow-run/d009b531-37f6-4fc3-8813-2d018fee354e</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:00:29.880 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - View at \u001b[94mhttps://actions-server-tyfar2-dblzzhtf5q-ew.a.run.app/flow-runs/flow-run/d009b531-37f6-4fc3-8813-2d018fee354e\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:00:30.530 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Created task run 'init inference_only_model-0' for task 'init inference_only_model'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:00:30.530 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Created task run 'init inference_only_model-0' for task 'init inference_only_model'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:00:30.533 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Executing 'init inference_only_model-0' immediately...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:00:30.533 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Executing 'init inference_only_model-0' immediately...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:00:31.392 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'init inference_only_model-0' - We have to download the TabPFN, as there is no checkpoint at  /Users/tylerfarnan/zk_tabpfn/zk_tabpfn/TabPFN/tabpfn/models_diff/prior_diff_real_checkpoint_n_0_epoch_42.cpkt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:00:31.392 | \u001b[36mINFO\u001b[0m    | Task run 'init inference_only_model-0' - We have to download the TabPFN, as there is no checkpoint at  /Users/tylerfarnan/zk_tabpfn/zk_tabpfn/TabPFN/tabpfn/models_diff/prior_diff_real_checkpoint_n_0_epoch_42.cpkt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:00:31.396 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'init inference_only_model-0' - It has about 100MB, so this might take a moment.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:00:31.396 | \u001b[36mINFO\u001b[0m    | Task run 'init inference_only_model-0' - It has about 100MB, so this might take a moment.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:52.717 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'init inference_only_model-0' - (inf, inf, TransformerModel(\n",
       "  (transformer_encoder): TransformerEncoderDiffInit(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Linear(in_features=100, out_features=512, bias=True)\n",
       "  (y_encoder): Linear(in_features=1, out_features=512, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       "))\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:52.717 | \u001b[36mINFO\u001b[0m    | Task run 'init inference_only_model-0' - (inf, inf, TransformerModel(\n",
       "  (transformer_encoder): TransformerEncoderDiffInit(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Linear(in_features=100, out_features=512, bias=True)\n",
       "  (y_encoder): Linear(in_features=1, out_features=512, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       "))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:53.028 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'init inference_only_model-0' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:53.028 | \u001b[36mINFO\u001b[0m    | Task run 'init inference_only_model-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:53.728 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Created task run 'generate_onnx_model-0' for task 'generate_onnx_model'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:53.728 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Created task run 'generate_onnx_model-0' for task 'generate_onnx_model'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:53.731 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Executing 'generate_onnx_model-0' immediately...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:53.731 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Executing 'generate_onnx_model-0' immediately...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerfarnan/zk_tabpfn/zk_tabpfn/TabPFN/tabpfn/transformer.py:111: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor([], device=x_src.device)\n",
      "/Users/tylerfarnan/zk_tabpfn/zk_tabpfn/TabPFN/tabpfn/transformer.py:112: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  global_src = torch.tensor([], device=x_src.device) if self.global_att_embeddings is None else \\\n",
      "/Users/tylerfarnan/anaconda3/lib/python3.11/site-packages/torch/onnx/symbolic_opset9.py:2174: FutureWarning: 'torch.onnx.symbolic_opset9._cast_Bool' is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.\n",
      "  return fn(g, to_cast_func(g, input, False), to_cast_func(g, other, False))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:56.573 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'generate_onnx_model-0' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:56.573 | \u001b[36mINFO\u001b[0m    | Task run 'generate_onnx_model-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:56.792 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Created task run 'get_dataset-0' for task 'get_dataset'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:56.792 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Created task run 'get_dataset-0' for task 'get_dataset'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:56.795 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Executing 'get_dataset-0' immediately...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:56.795 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Executing 'get_dataset-0' immediately...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerfarnan/zk_tabpfn/zk_tabpfn/TabPFN/tabpfn/datasets/__init__.py:50: FutureWarning: Support for `output_format` of 'dict' will be removed in 0.15 and pandas dataframes will be returned instead. To ensure your code will continue to work, use `output_format`='dataframe'.\n",
      "  openml_list = openml.datasets.list_datasets(dids)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:58.342 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_dataset-0' - Number of datasets: 1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:58.342 | \u001b[36mINFO\u001b[0m    | Task run 'get_dataset-0' - Number of datasets: 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:58.348 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_dataset-0' - Loading balance-scale 11 ..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:58.348 | \u001b[36mINFO\u001b[0m    | Task run 'get_dataset-0' - Loading balance-scale 11 ..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerfarnan/zk_tabpfn/zk_tabpfn/TabPFN/tabpfn/datasets/__init__.py:8: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  dataset = openml.datasets.get_dataset(did)\n",
      "/Users/tylerfarnan/zk_tabpfn/zk_tabpfn/TabPFN/tabpfn/datasets/__init__.py:9: FutureWarning: Support for `dataset_format='array'` will be removed in 0.15,start using `dataset_format='dataframe' to ensure your code will continue to work. You can use the dataframe's `to_numpy` function to continue using numpy arrays.\n",
      "  X, y, categorical_indicator, attribute_names = dataset.get_data(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:58.408 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_dataset-0' - Evaluation dataset name: balance-scale shape torch.Size([625, 4])\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:58.408 | \u001b[36mINFO\u001b[0m    | Task run 'get_dataset-0' - Evaluation dataset name: balance-scale shape torch.Size([625, 4])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:58.409 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_dataset-0' - torch.Size([625, 4])\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:58.409 | \u001b[36mINFO\u001b[0m    | Task run 'get_dataset-0' - torch.Size([625, 4])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:58.410 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_dataset-0' - torch.Size([625])\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:58.410 | \u001b[36mINFO\u001b[0m    | Task run 'get_dataset-0' - torch.Size([625])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:58.410 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_dataset-0' - 312\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:58.410 | \u001b[36mINFO\u001b[0m    | Task run 'get_dataset-0' - 312\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:58.650 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_dataset-0' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:58.650 | \u001b[36mINFO\u001b[0m    | Task run 'get_dataset-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:58.876 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Created task run 'fit-0' for task 'fit'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:58.876 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Created task run 'fit-0' for task 'fit'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:58.880 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Executing 'fit-0' immediately...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:58.880 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Executing 'fit-0' immediately...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:59.319 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'fit-0' - Setting up TabPFN with training data context\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:59.319 | \u001b[36mINFO\u001b[0m    | Task run 'fit-0' - Setting up TabPFN with training data context\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:59.641 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'fit-0' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:59.641 | \u001b[36mINFO\u001b[0m    | Task run 'fit-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:59.855 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Created task run 'predict-0' for task 'predict'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:59.855 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Created task run 'predict-0' for task 'predict'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:01:59.857 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Executing 'predict-0' immediately...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:01:59.857 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Executing 'predict-0' immediately...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:02:00.362 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'predict-0' - prediction!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:02:00.362 | \u001b[36mINFO\u001b[0m    | Task run 'predict-0' - prediction!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerfarnan/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x2df5ddcc0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:02:01.230 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'predict-0' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:02:01.230 | \u001b[36mINFO\u001b[0m    | Task run 'predict-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:02:01.458 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Created task run 'eval-0' for task 'eval'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:02:01.458 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Created task run 'eval-0' for task 'eval'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:02:01.461 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Executing 'eval-0' immediately...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:02:01.461 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Executing 'eval-0' immediately...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:02:01.967 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'eval-0' - AUC 0.9799443358478483\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:02:01.967 | \u001b[36mINFO\u001b[0m    | Task run 'eval-0' - AUC 0.9799443358478483\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x2ccc56660>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:02:02.228 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'eval-0' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:02:02.228 | \u001b[36mINFO\u001b[0m    | Task run 'eval-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:02:02.470 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Action run 'hilarious-pegasus' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>('All states completed.')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:02:02.470 | \u001b[36mINFO\u001b[0m    | Action run 'hilarious-pegasus' - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x2b80a40c0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `TabPFNClassifier`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `tuple`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `TabPFNClassifier`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `ndarray`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupported Transpiler Operators\n",
    "\n",
    "- {'IsInf', 'Mod', 'Less', 'Cast', 'Shape', 'Greater', 'Erf', 'Pow', 'Transpose', 'Where', 'Sqrt', 'ReduceMean', 'IsNaN', 'Gather', 'Slice'}\n",
    "- Many repeats across transformer encoders, might make more sense to add these to transpiler instead of manually implementing?\n",
    "\n",
    "#### Not yet implemented in Orion...\n",
    "\n",
    "- {'Cast', 'Shape'}\n",
    "- Maybe the orion tensor trait shape attribute could simply be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps!!\n",
    "\n",
    "- Implement these Operators in the transpiler :)\n",
    "- Benchmark ZK overhead for TabPFN \n",
    "- Test out TabPFN on Giza Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
